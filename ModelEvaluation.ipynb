{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84ea3b21",
   "metadata": {},
   "source": [
    "# Classification Matrics.\n",
    "  - Classification metrics are used to evaluate the performance of a classification model by comparing predicted values with actual values.\n",
    "\n",
    "  ## Accuracy.\n",
    "     - Measures overall correctness of the model.\n",
    "     - Accuracy=TP+TN/FP+FN+TP+TN\n",
    "\n",
    "  ## Precision.\n",
    "     - Out of predicted positives, how many are actually positive.\n",
    "     - Precision=TP/TP+FP\n",
    "\n",
    "  ## Re-Call.(Sensitivity)\n",
    "     - Out of actual positives, how many were correctly predicted.\n",
    "     - Recall= TP/TP+FN\n",
    "\n",
    "  ## F1-Score\n",
    "     - Harmonic mean of Precision and Recall.\n",
    "     - F1-Score= 2 * Precision * Recall /  Precision + Recall\n",
    "\n",
    "  ## ROC Curve & AUC\n",
    "    - ROC Curve ‚Üí TPR vs FPR\n",
    "    - AUC ‚Üí Area under ROC curve\n",
    "    - üìå Higher AUC = Better model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9baffef",
   "metadata": {},
   "source": [
    "# When to Use Which Metric?\n",
    "\n",
    "| Situation         | Best Metric |\n",
    "| ----------------- | ----------- |\n",
    "| Balanced data     | Accuracy    |\n",
    "| Spam detection    | Precision   |\n",
    "| Medical diagnosis | Recall      |\n",
    "| Imbalanced data   | F1-Score    |\n",
    "| Model comparison  | AUC         |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95a6a7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,  confusion_matrix  \n",
    "# How well does a classification model perform? import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ead4a585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7142857142857143\n",
      "Precision: 0.75\n",
      "Recall: 0.75\n",
      "F1 Score: 0.75\n"
     ]
    }
   ],
   "source": [
    "#True Answers(What actually is)\n",
    "X=[1,0,1,1,0,1,0]\n",
    "\n",
    "#predicted Answers(What model predicted)\n",
    "y=[1,0,0,1,0,1,1]\n",
    "\n",
    "print(\"Accuracy:\",accuracy_score(X,y))\n",
    "print(\"Precision:\",precision_score(X,y))\n",
    "print(\"Recall:\",recall_score(X,y))\n",
    "print(\"F1 Score:\",f1_score(X,y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb4db34",
   "metadata": {},
   "source": [
    "# Confusion Matrics.\n",
    "  - A confusion matrix is a table used to evaluate the performance of a classification model by comparing actual values with predicted values.\n",
    "\n",
    "  - Binary Classification Confusion Matrix\n",
    "    - | Actual \\ Predicted | Positive           | Negative           |\n",
    "      | ------------------ | -------------------| ------------------ |\n",
    "      | Positive           | TP(True Positive)  | FN(False Negative) |\n",
    "      | Negative           | FP(False Positive) | TN (True Negative) |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc275ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrics:\n",
      " [[2 1]\n",
      " [1 3]]\n"
     ]
    }
   ],
   "source": [
    "X = [1,0,1,1,0,1,0]\n",
    "y = [1,0,0,1,0,1,1]\n",
    "print(\"Confusion Matrics:\\n\",confusion_matrix(X,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63db1f5",
   "metadata": {},
   "source": [
    "# Regression Matrics.\n",
    "  - Regression metrics are used to evaluate regression models (models that predict continuous values, like price,\n",
    "     marks, temperature).\n",
    "    \n",
    "    ### 1. MAE(Mean Absolute Error).\n",
    "        - Average of absolute errors.\n",
    "        - MAE= 1/n ‚àë|y actual - y predicted|\n",
    "        - Example:\n",
    "                 - Actual = 100, Predicted = 90 ‚Üí Error = 10\n",
    "\n",
    "    ### 2. Mean Squared Error (MSE)\n",
    "        - Average of squared errors.\n",
    "        - MSE =  1/n ‚àë (|y actual - y predicted|)^2\n",
    "\n",
    "    ### 3. Root Mean Squared Error (RMSE)\n",
    "        - Square root of MSE\n",
    "        - RMSE = ‚àöMSE \n",
    "\n",
    "## Quick Comparison Table.\n",
    "   - | Metric | Best When           | Sensitive to Outliers |\n",
    "     | ------ | ------------------- | --------------------- |\n",
    "     | MAE    | Balanced error      | ‚ùå No                  |\n",
    "     | MSE    | Penalize big errors | ‚úÖ Yes                 |\n",
    "     | RMSE   | Interpretability    | ‚úÖ Yes                 |\n",
    "\n",
    "## When to Use What?\n",
    "   - MAE ‚Üí simple & robust\n",
    "   - RMSE ‚Üí large errors matter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e60974c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 7.5\n",
      "Mean Squared Error: 62.5\n",
      "Root Mean Squared Error: 7.905694150420948\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, root_mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Real Scores\n",
    "Real_Score=[90,60,80,100]\n",
    "\n",
    "# Predicted Scores\n",
    "Predicted_Score=[85,70,70,95]\n",
    "\n",
    "print(\"Mean Absolute Error:\", mean_absolute_error(Real_Score, Predicted_Score))\n",
    "print(\"Mean Squared Error:\", mean_squared_error(Real_Score, Predicted_Score))\n",
    "print(\"Root Mean Squared Error:\", np.sqrt(mean_squared_error(Real_Score, Predicted_Score))) # np.sqrt is used to calculate the square root of the mean squared error to get the root mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa44728",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
