{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84ea3b21",
   "metadata": {},
   "source": [
    "# Classification Matrics.\n",
    "  - Classification metrics are used to evaluate the performance of a classification model by comparing predicted values with actual values.\n",
    "\n",
    "  ## Accuracy.\n",
    "     - Measures overall correctness of the model.\n",
    "     - Accuracy=TP+TN/FP+FN+TP+TN\n",
    "\n",
    "  ## Precision.\n",
    "     - Out of predicted positives, how many are actually positive.\n",
    "     - Precision=TP/TP+FP\n",
    "\n",
    "  ## Re-Call.(Sensitivity)\n",
    "     - Out of actual positives, how many were correctly predicted.\n",
    "     - Recall= TP/TP+FN\n",
    "\n",
    "  ## F1-Score\n",
    "     - Harmonic mean of Precision and Recall.\n",
    "     - F1-Score= 2 * Precision * Recall /  Precision + Recall\n",
    "\n",
    "  ## ROC Curve & AUC\n",
    "    - ROC Curve â†’ TPR vs FPR\n",
    "    - AUC â†’ Area under ROC curve\n",
    "    - ðŸ“Œ Higher AUC = Better model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9baffef",
   "metadata": {},
   "source": [
    "# When to Use Which Metric?\n",
    "\n",
    "| Situation         | Best Metric |\n",
    "| ----------------- | ----------- |\n",
    "| Balanced data     | Accuracy    |\n",
    "| Spam detection    | Precision   |\n",
    "| Medical diagnosis | Recall      |\n",
    "| Imbalanced data   | F1-Score    |\n",
    "| Model comparison  | AUC         |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95a6a7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix \n",
    "# How well does a classification model perform? import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead4a585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7142857142857143\n",
      "Precision: 0.75\n",
      "Recall: 0.75\n",
      "F1 Score: 0.75\n",
      "Confusion Matrix:\n",
      " [[2 1]\n",
      " [1 3]]\n"
     ]
    }
   ],
   "source": [
    "#True Answers(What actually is)\n",
    "X=[1,0,1,1,0,1,0]\n",
    "\n",
    "#predicted Answers(What model predicted)\n",
    "y=[1,0,0,1,0,1,1]\n",
    "\n",
    "print(\"Accuracy:\",accuracy_score(X,y))\n",
    "print(\"Precision:\",precision_score(X,y))\n",
    "print(\"Recall:\",recall_score(X,y))\n",
    "print(\"F1 Score:\",f1_score(X,y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc275ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
